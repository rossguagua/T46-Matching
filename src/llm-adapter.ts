import { GoogleGenerativeAI } from '@google/generative-ai'
import OpenAI from 'openai'

export interface LLMResponse {
  text: string
  error?: string
}

export interface LLMConfig {
  provider: 'gemini' | 'deepseek' | 'openai'
  model: string
  temperature: number
  maxTokens?: number
  apiKey: string
}

class LLMAdapter {
  async generateContent(prompt: string, config: LLMConfig): Promise<LLMResponse> {
    try {
      if (config.provider === 'deepseek') {
        const deepseekAI = new OpenAI({
          baseURL: 'https://api.deepseek.com/v1',
          apiKey: config.apiKey,
          dangerouslyAllowBrowser: true
        })
        
        const response = await deepseekAI.chat.completions.create({
          model: config.model,
          messages: [{ role: 'user', content: prompt }],
          temperature: config.temperature,
          max_tokens: config.maxTokens || 2000,
        })

        const text = response.choices[0]?.message?.content || ''
        console.log(`ğŸ¤– DeepSeekå“åº”é•¿åº¦: ${text.length}`)
        return { text }
      } 
      else if (config.provider === 'gemini') {
        const geminiAI = new GoogleGenerativeAI(config.apiKey)
        const model = geminiAI.getGenerativeModel({ 
          model: config.model,
          generationConfig: { 
            temperature: config.temperature,
            maxOutputTokens: config.maxTokens 
          }
        })
        
        const result = await model.generateContent(prompt)
        const response = result.response
        
        if (!response.candidates || response.candidates.length === 0) {
          throw new Error(`Geminiè¿”å›ç©ºå“åº”: ${JSON.stringify(response)}`)
        }
        
        const candidate = response.candidates[0]
        if (candidate.finishReason === 'SAFETY') {
          throw new Error('Geminiå“åº”è¢«å®‰å…¨è¿‡æ»¤å™¨æ‹¦æˆª')
        }
        
        const text = response.text()
        console.log(`ğŸ¤– Geminiå“åº”é•¿åº¦: ${text.length}`)
        return { text }
      } 
      else if (config.provider === 'openai') {
        const openaiAI = new OpenAI({
          apiKey: config.apiKey,
          dangerouslyAllowBrowser: true
        })
        
        const response = await openaiAI.chat.completions.create({
          model: config.model,
          messages: [{ role: 'user', content: prompt }],
          temperature: config.temperature,
          max_tokens: config.maxTokens || 2000,
        })

        const text = response.choices[0]?.message?.content || ''
        console.log(`ğŸ¤– OpenAIå“åº”é•¿åº¦: ${text.length}`)
        return { text }
      }
      else {
        throw new Error(`ä¸æ”¯æŒçš„LLMæä¾›å•†: ${config.provider}`)
      }
    } catch (error) {
      console.error(`âŒ LLM APIè°ƒç”¨å¤±è´¥è¯¦æƒ…:`)
      console.error(`  æä¾›å•†: ${config.provider}`)
      console.error(`  æ¨¡å‹: ${config.model}`)
      console.error(`  é”™è¯¯ç±»å‹: ${error instanceof Error ? error.constructor.name : typeof error}`)
      console.error(`  é”™è¯¯ä¿¡æ¯: ${error instanceof Error ? error.message : String(error)}`)
      
      if (error instanceof Error && error.stack) {
        console.error(`  é”™è¯¯å †æ ˆ: ${error.stack}`)
      }
      
      // é’ˆå¯¹ä¸åŒé”™è¯¯ç±»å‹æä¾›æ›´æœ‰ç”¨çš„ä¿¡æ¯
      let friendlyMessage = error instanceof Error ? error.message : String(error)
      
      if (friendlyMessage.includes('401') || friendlyMessage.includes('Unauthorized')) {
        friendlyMessage = `APIå¯†é’¥æ— æ•ˆæˆ–å·²è¿‡æœŸ (${config.provider})`
      } else if (friendlyMessage.includes('429') || friendlyMessage.includes('quota')) {
        friendlyMessage = `APIé…é¢å·²ç”¨å®Œæˆ–è¯·æ±‚è¿‡äºé¢‘ç¹ (${config.provider})`
      } else if (friendlyMessage.includes('400') || friendlyMessage.includes('Bad Request')) {
        friendlyMessage = `è¯·æ±‚æ ¼å¼é”™è¯¯ï¼Œå¯èƒ½æ˜¯æ¨¡å‹åç§°ä¸æ­£ç¡® (${config.provider}: ${config.model})`
      } else if (friendlyMessage.includes('network') || friendlyMessage.includes('fetch')) {
        friendlyMessage = `ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥ (${config.provider})`
      } else if (friendlyMessage.includes('SAFETY')) {
        friendlyMessage = `Geminiå†…å®¹å®‰å…¨è¿‡æ»¤å™¨æ‹¦æˆªäº†å“åº”ï¼Œè¯·å°è¯•è°ƒæ•´è¾“å…¥å†…å®¹`
      } else if (friendlyMessage.includes('Resource has been exhausted')) {
        friendlyMessage = `Gemini APIé…é¢å·²è€—å°½ï¼Œè¯·æ£€æŸ¥é…é¢é™åˆ¶æˆ–ç¨åé‡è¯•`
      } else if (config.provider === 'gemini' && friendlyMessage.includes('models/')) {
        friendlyMessage = `Geminiæ¨¡å‹åç§°å¯èƒ½ä¸æ­£ç¡®: ${config.model}ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨`
      }
      
      return { 
        text: '', 
        error: friendlyMessage
      }
    }
  }

  getAvailableModels(): { [provider: string]: string[] } {
    return {
      deepseek: ['deepseek-v3'],
      gemini: ['gemini-2.5-pro', 'gemini-2.5-flash', 'gemini-2.5-flash-lite', 'gemini-1.5-pro', 'gemini-1.5-flash'],
      openai: ['gpt-4o', 'gpt-4o-mini']
    }
  }
}

export default LLMAdapter